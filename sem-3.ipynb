{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "\n",
    "<img src=\"img/batch_normalization.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczenia mogą być wykonywane niezależnie dla każdej z $d$ cech. Mamy\n",
    "\n",
    "$x^{(k)}\\in(x^{(1)}...x^{(d)})$ będący jedną konkretną cechą $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m$ - rozmiar batcha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem jest normalizacja danych wejściowych tak, aby w obrębie batcha zachowany były warunki: \n",
    "\n",
    "$E[x^{(k)}]=0$ \n",
    "\n",
    "$Var[x^{(k)}]=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy:\n",
    "$\\mathcal{B}=\\{x_{1...m}\\}$ - zbiór danych wejściowych dla rozważanego neuronu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do każdej warstwy przekazujemy zmodyfikowane wejście:\n",
    "\n",
    "$\\widehat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]+\\epsilon}}$\n",
    "\n",
    "gdzie $\\epsilon=const$ został dodany dla stabilności numerycznej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyjście warstwy wynosi:\n",
    "\n",
    "$y^{(k)}=\\gamma^{(k)}\\widehat{x}^{(k)} + \\beta^{(k)}$\n",
    "\n",
    "Algorytm musi nauczyć się parametrów $\\gamma^{(k)}$ oraz $\\beta^{(k)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch Normalizing Transform\n",
    "\n",
    "$BN_{\\gamma,\\beta}:x_{1...m}\\rightarrow y_{1...m}$ - Batch Normalizing Transform\n",
    "\n",
    "(Parametr (k) pominięty)\n",
    "\n",
    "1. $\\mu_{\\mathcal{B}}\\leftarrow\\frac{1}{m}\\sum\\limits_{i=1}^{m}{x_i}$\n",
    "\n",
    "2. $\\sigma_{\\mathcal{B}}^2\\leftarrow\\frac{1}{m}\\sum\\limits_{i=1}^{m}{(x_i-\\mu_{\\mathcal{B}})^2}$\n",
    "\n",
    "3. $\\widehat{x}_i\\leftarrow\\frac{x_i-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2+\\epsilon}}$\n",
    "\n",
    "4. $y_i\\leftarrow\\gamma\\widehat{x}_i+\\beta\\equiv BN_{\\gamma,\\beta}(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm uczenia\n",
    "\n",
    "**Input:** Network $N$ with trainable parameters $\\Theta$;subset of activations $\\{x^{(k)}\\}^K_{k=1}$\n",
    "\n",
    "**Output:** Batch-normalized network for inference, $N^{inf}_{BN}$\n",
    "\n",
    "1. $N^{tr}_{BN}\\leftarrow N$\n",
    "2. **for** $k=1...K$ **do*\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;Add transformation $y^{(k)}=BN_{\\gamma^{k},\\beta^{k}}(x^{(k)})$ to $N^{tr}_{BN}$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Modify each layer in $N^{tr}_{BN}$ with input $x^{(k)}$ to take $y^{(k)}$ instead\n",
    "5. **end for**\n",
    "6. Train $N^{tr}_{BN}$ to optimize the parameters $\\Theta \\cup \\{\\gamma^{(k)},\\beta^{(k)}\\}^K_{k=1}$\n",
    "7. $N^{inf}_{BN}\\leftarrow N^{tr}_{BN}$\n",
    "8. **for** $k=1...K$ **do**\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;Process multiple training mini-batches $\\mathcal{B}$, each of size $m$, and average over them: $$\\begin{align} E[x^{(k)}]\\leftarrow E_{\\mathcal{B}}[\\mu_{\\mathcal{B}}] \\\\Var[x^{(k)}]\\leftarrow \\frac{m}{m-1} E_{\\mathcal{B}}[\\sigma_{\\mathcal{B}}^2]\n",
    "\\end{align}\n",
    "$$\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp; In $N^{inf}_{BN}$, replace the transform $y=BN_{\\gamma,\\beta}(x)$ with: \n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(k)}=\\frac{\\gamma^{(k)}}{\\sqrt{Var[x^{(k)}]+\\epsilon}}\\cdot x^{(k)} +(\\beta^{(k)}-\\frac{\\gamma^{(k)} E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]+\\epsilon}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "11.**end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy odzyskać oryginalne aktywacje poprzez użycie wartości parametrów\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\gamma^{(k)} = \\sqrt{Var[x^{(k)}]} \\\\\n",
    "\\beta^{(k)}=E[x^{(k)}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Cechy\n",
    "\n",
    "* Przyspiesza uczenie sieci\n",
    "* Stabilizuje proces nauczania\n",
    "* Zmniejsza wpływ warunków początkowych na proces nauczania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch Normalization w DCGAN\n",
    "\n",
    "Normalizacja używana osobno w warstwach **generatora** oraz **dyskryminatora**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i=\n",
    "\\begin{cases}\n",
    "x_i & \\quad \\text{if } x_i \\geq 0 \\\\\n",
    "0 & \\quad \\text{if } x_i<0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Użyta w warstwach generatora z wyjątkiem warstwy wyjściowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i=\n",
    "\\begin{cases}\n",
    "x_i & \\quad \\text{if } x_i \\geq 0 \\\\\n",
    "\\frac{x_i}{a_i} & \\quad \\text{if } x < 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "gdzie $a_i\\cup(1,+\\infty)$\n",
    "\n",
    "Użyta w warstwach dyskriminatora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Badania\n",
    "\n",
    "Użyto 2 zbiorów z bazy LSUN\n",
    " * Zbiór 3 milionów zdjęć sypialni\n",
    " * Zbiór 350 tysięcy zdjęć twarzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
